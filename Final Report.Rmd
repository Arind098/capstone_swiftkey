---
title: Word Prediction App with R
output: 
    html_document:
      highlight: espresso
      theme: flatly
---

```{r Options, echo= F, warning= F, message= F}
# Libraries and options ####
library(knitr)
```
  
  
#### Introduction   
   
This project was the final part of a 10 course [Data Science track by John Hopkin’s University on Coursera](https://www.coursera.org/specializations/jhu-data-science). It was done as an industry partnership with [SwiftKey](https://swiftkey.com/en). The job was to clean and analyze large corpus of unstructured text and build a word prediction model and use it in a web application. This page is my verbal account of the project.

If you are interested, you can check the following links too:
*Word Predictor Application:*
*Repository:*  

#### The Data  

The data is from a corpus called [HC Corpora](http://www.corpora.heliohost.org/aboutcorpus.html). It consists of text files collected from publicly available sources by a web crawler. I used english language files that were gathered from Twitter and different blogs and news sources. This combination should give a rather good mix of general language used today. 

The data are large text files. Over 4 million lines combined. Unix wordcount gives 102,081,616 individual words. They are not in a sequential order, eg. the lines in the "Blogs" - file are not complete posts and the same post does not continue in the next line.  

#### Text Transformations  
The process of transforming raw text to useful units for text analysis is called [tokenization](https://en.wikipedia.org/wiki/Lexical_analysis). What kind of transformations are needed is an important choice. In the case of word prediction, it is probably the most important step.
 
**Here is a list of transformations that I used (or did not use) and my musings:**

0. Word Stemming
	- In many NLP tasks you [stem](https://en.wikipedia.org/wiki/Stemming) the words, which means reducing inflected or derived word to its basic part (ie. connection, connected and connecting, would all become connect)
	- I did not do this, because it would greatly reduce the predictive power  
	
1. Split sentences to individual lines
	- Data is in rather random order and for word prediction, sentence is the reasonable unit
	- Downside is that you lose lot of context that could be used by elaborate algorithms  
	
2. All text to lower case
	- Removes the problem of beginning of sentence words being “different” than the others.
	- Combined with punctuation, this information	could be used for prediction
	- It would be good to ignore capital letters in the beginning of sentence, but keep them elsewhere to catch names and acronyms correctly  
	
3. Remove numbers
	- Remove tokens that consist only of numbers, but not words that start with digits, e.g. 2day)
	- Numbers are hard to predict or use in word prediction  
	
4. Remove punctuation
	-  With simple ngram-model punctuation causes too many sequences
	- Could be useful with advanced algorhitms combined with other features  
	
5. Remove separators
	- Spaces and variations of spaces, plus tab, newlines, and anything else in the Unicode "separator" category
	- No use for prediction  
	
6. Remove Twitter characters
	- ie.(@ and #)
	- Better to capture only the words  
	
7. Profanity filtering
	- Only filtered the ”Seven Words You Can Never Say on Television" and only in their basic form
	- Trying to get this 100% right would be a daunting and interesting task given the human creativity on the subject  
	
#### Prediction Model  

To keep the scope of the project managable, I only concidered so called Markov models. They are a class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past. I based my model on the stupid backoff -algorithm. Despite its name, it actually performs quite well given very large data. Actually, almost as well as some more complex models.

Stupid backoff -algorithm centers around [n-grams](https://en.wikipedia.org/wiki/N-gram). They mean contiguous word sequences of length n. Selection of the size depends on the genre of text you are trying to predict. Higher n-grams are not always preferable for prediction. Also the computing and storage needs grow expotentially with that parameter. I chose to use n-grams of lengts one, two and three. This means that the predictions can be based on maximum two previous words.

*Basic Idea of the Algorithm used:*

1. Take the input and use the same text transformations as for the training data and return last two words.
2. Search for two first input words in the 3-grams training data and if matched, predict the third word. If no match, then next step.
3. Search with only the last input word in the first word of 2-grams training data. If matched, predict the second word. If no match, then next step.
4. Predict the most common words in the 1-gram data.

*For more in-depth look, [here](https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf) is a good summary on basic text prediction.*  

#### Testing and Accuracy  

I first set aside 20% of the data to use as the validation set and the rest was used when building the model. Because of computing limitations, I ended up using only 10% of the validation set. I used my input function (same transformations as with the training data). The resulting 1-grams and 2-grams were given to the prediction algorithm, and checked how many times it got the prediction right.  

**Here are the results with five, tree and one next word suggestions per prediction per n-gram:**
```{r, echo= F}
load('/Users/r2/MOOC/Data Science - Specialization/10. Capstone - NLP/accuSummary1.Rdata')
print(accuSummary)
```
  
So called stopwords (ie. “the”, “to” etc) are very prevalent in English. These make up a high frequency of words in test and validation sets. This probably inflates the tested accuracy compared to the perceived usefulness in real world. Usually you are more interested if the application can predict words that carry more meaning and are harder to type.  

#### Notes on R Packages Used  

My first approach was to use the gold standard package for NLP tasks, the [tm](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf). It turned out to be quite slow with this magnitude of data, even when using samples of n/1000. Also its data structures were complicating the task. I then switched to newer [quanteda](https://cran.r-project.org/web/packages/quanteda/vignettes/quickstart.html) package. It proofed to be faster and more user friendly for basic tasks needed in this assignment.

For more efficient computing, one could use markov chains, hashing or some other more elaborate methods. I wanted to keep it simple and see and play with the data in easy format throughout the exercise. I did most of the work in data table -format with [dplyr](https://cran.r-project.org/web/packages/dplyr/README.html). It was suprisingly fast and was not a bottleneck  when the data manipulation steps were in right sequence.  
  
  
  

